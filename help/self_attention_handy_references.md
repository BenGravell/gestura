1. Vaswani, et al. (2017). [Attention Is All You Need](https://arxiv.org/abs/1706.03762).
    - Introduced the Transformer architecture which is the foundation of models like BERT, GPT, etc.

2. Jay Alammar's [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/).
    - A visual guide to the Transformer model.

3. [A Gentle Introduction to Transformers](https://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html) from the [Dive into Deep Learning book](https://d2l.ai/index.html).

4. Lilian Weng's [Attention? Attention!](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html).
    - A comprehensive blog post detailing various attention mechanisms.

5. [The Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer/) by Alexander Rush.
    - The Transformer model implementation with annotations.
